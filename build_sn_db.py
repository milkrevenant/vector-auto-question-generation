import os, glob, json, chromadb, tiktoken
import hashlib, re
 # â”€â”€ ê·¸ë£¹ í•´ì‹œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def canonical_passage(item: dict) -> str:
    """passage ë˜ëŠ” context_boxë¥¼ ê³µë°± 1ì¹¸ìœ¼ë¡œ ì •ê·œí™”í•´ ë°˜í™˜"""
    txt = (item.get("passage") or item.get("context_box") or "")
    return " ".join(txt.split())

def passage_hash(item: dict, length: int = 12) -> str:
    """
    ë™ì¼ ì§€ë¬¸ì´ë©´ íŒŒì¼ëª…ì´ ë‹¬ë¼ë„ ë™ì¼ í•´ì‹œë¥¼ ì–»ë„ë¡ SHAâ€‘1 ê¸°ë°˜ ê·¸ë£¹í‚¤ ìƒì„±.
    ê¸°ë³¸ 12ì(48bit) â†’ ì¶©ëŒ í™•ë¥  1/2^48 â‰ˆ 1.4eâ€‘14
    """
    text = canonical_passage(item)
    return hashlib.sha1(text.encode("utf-8")).hexdigest()[:length]
from openai import OpenAI

# â”€â”€ â¶ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SRC_DIR = "/Users/stillclie_mac/Documents/ug/snoriginal/db"
DB_PATH = "./sn_csat.db"               # DuckDB íŒŒì¼
COL_NAME = "sn_csat_openai"

# â”€â”€ â· ëª¨ë¸ & ë„êµ¬ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
EMBED_MODEL = os.environ.get("OPENAI_EMBED_MODEL", "text-embedding-3-large")
print(f"ğŸ”§  Using embedding model: {EMBED_MODEL}")

openai  = OpenAI()
client  = chromadb.PersistentClient(path=DB_PATH)
col     = client.get_or_create_collection(
             COL_NAME, metadata={"hnsw:space":"cosine"}
         )

def merge_text(item: dict) -> str:
    """
    ì§€ë¬¸ + ë¬¸ì œ + ì„ íƒì§€ë¥¼ í•œ ë¬¸ìì—´ë¡œ
    None ì´ë‚˜ ëˆ„ë½ í•„ë“œëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•´ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•œë‹¤.
    """
    passage  = item.get("passage") or item.get("context_box") or ""
    question = item.get("question") or ""
    choices  = " ".join(opt.get("text", "") for opt in item.get("options", []))
    return f"{passage}\n{question}\n{choices}"

# â”€â”€ â¸ ëª¨ë“  JSON íŒŒì¼ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
files = sorted(glob.glob(os.path.join(SRC_DIR, "*.json")))
print(f"ğŸ”  Found {len(files)} JSON files.")

ids, docs, metas = [], [], []
for path in files:
    with open(path, encoding="utf-8") as f:
        item = json.load(f)
    # passageê°€ ì—†ìœ¼ë©´ context_boxë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸
    if not (item.get("passage") or item.get("context_box")) or not item.get("question"):
        print(f"âš ï¸  Skip {path} (missing passage/context_box ë˜ëŠ” question)")
        continue

    ids.append(item["id"])
    docs.append(merge_text(item))
    # ë©”íƒ€ë°ì´í„°ì—ì„œ None ê°’ì„ ì œê±°(ChromaëŠ” Noneì„ í—ˆìš©í•˜ì§€ ì•ŠìŒ)
    clean_meta = {
        k: v for k, v in item.items()
        if k not in ("passage", "question", "options") and v is not None
    }
    # â€• ê·¸ë£¹ í•´ì‹œ ì¶”ê°€ â€•
    clean_meta["group"] = passage_hash(item)
    metas.append(clean_meta)

# â”€â”€ â¹ OpenAI ì„ë² ë”© (128ê°œì”© ë°°ì¹˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê°¯ìˆ˜ ìƒê´€ì—†ìŒ
def embed(batch):
    res = openai.embeddings.create(
        model=EMBED_MODEL,
        input=batch
    )
    return [d.embedding for d in res.data]

BATCH = 128
embs = []
for i in range(0, len(docs), BATCH):
    embs.extend(embed(docs[i:i+BATCH]))
    print(f"  â†’ Embedded {len(embs)}/{len(docs)}")

# â”€â”€ âº Chroma ì»¬ë ‰ì…˜ì— ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
col.add(ids=ids, documents=docs, embeddings=embs, metadatas=metas)
print(f"âœ…  {len(ids)} items stored in {DB_PATH}:{COL_NAME}")