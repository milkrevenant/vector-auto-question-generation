# make_new_items.py  â—†â—†â—† 2025â€‘07â€‘24 ìˆ˜ì • â—†â—†â—†
from pathlib import Path
import os, re, json, textwrap, datetime
import chromadb
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¶ ì„¤ì •ê°’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DB_PATH      = Path(os.getenv("SN_DB_PATH", "./sn_csat.db"))
EMBED_MODEL  = os.getenv("OPENAI_EMBED_MODEL", "text-embedding-3-large")
CHAT_MODEL   = os.getenv("OPENAI_CHAT_MODEL",  "gpt-4.1-mini")
N_QUESTIONS  = int(os.getenv("SN_NUM_Q",  "3"))
MAX_EXAMPLES = int(os.getenv("SN_MAX_EX", "2"))
TEMP         = float(os.getenv("SN_TEMP",  "0.7"))

openai = OpenAI()
col = chromadb.PersistentClient(path=str(DB_PATH)).get_collection("sn_csat_openai")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â· í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed(text: str):
    return openai.embeddings.create(model=EMBED_MODEL, input=text).data[0].embedding

def build_prompt(passage: str, examples: list[dict]) -> list[dict]:
    ex_blocks = []
    for ex in examples[:MAX_EXAMPLES]:
        meta, doc = ex["meta"], ex["doc"]
        q_type  = meta.get("q_type")  or meta.get("type") or "ìœ í˜• ë¯¸ìƒ"
        opt_fmt = meta.get("opt_type") or "5ì§€ì„ ë‹¤"
        ex_blocks.append(f"[ì˜ˆì‹œ ë¬¸ì œ] ìœ í˜•={q_type} / ì„ íƒì§€={opt_fmt}\n{doc}")

    sys = textwrap.dedent(f"""
        ë„ˆëŠ” **í•œêµ­ ìˆ˜ëŠ¥ êµ­ì–´ ì¶œì œìœ„ì›**ì´ë‹¤.
        â€£ ì•„ë˜ â€˜ìƒˆ ì§€ë¬¸â€™ì„ ê·¸ëŒ€ë¡œ í™œìš©í•´ **{N_QUESTIONS}ê°œ 5ì§€ì„ ë‹¤** ë¬¸ì œë¥¼ ë§Œë“ ë‹¤.
        â€£ ìµœì†Œ **1â€¯ë¬¸ì œ ì´ìƒ**ì€ (ë³´ê¸°) ìë£Œë¥¼ ì œì‹œí•´ì•¼ í•œë‹¤.
        â€£ ë‚œì´ë„ëŠ” ìˆ˜ëŠ¥ êµ­ì–´ â€˜ìƒâ€™(ìƒìœ„â€¯20â€¯%) ìˆ˜ì¤€.
        â€£ ê° ë¬¸í•­ë§ˆë‹¤ í˜¼ë™ ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ì˜¤ë‹µ, **ì •ë‹µ**, ê°„ë‹¨ **í•´ì„¤**ì„ í¬í•¨.
        â€£ ì¶œë ¥ì€ ë‹¤ìŒ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤. ë¶ˆí•„ìš”í•œ í•„ë“œëŠ” ë„£ì§€ ë§ ê²ƒ.
          [
            {{
              "ë¬¸ì œ": "...",
              "ë³´ê¸°": "ã‰  ... / ã‰¡ ...",      # (ë³´ê¸°) ì—†ëŠ” ë¬¸í•­ì€ ìƒëµ
              "ì„ íƒì§€": ["â‘  ...", "â‘¡ ...", "â‘¢ ...", "â‘£ ...", "â‘¤ ..."],
              "ì •ë‹µ": "â‘¢",
              "í•´ì„¤": "..."
            }}, â€¦
          ]
        ë°˜ë“œì‹œ **ìœ íš¨í•œ JSONë§Œ** ë°˜í™˜í•œë‹¤.
    """).strip()

    user = "# ìƒˆ ì§€ë¬¸\n" + passage.strip() + "\n\n" + "\n\n".join(ex_blocks)

    return [
        {"role": "system", "content": sys},
        {"role": "user",   "content": user}
    ]

def generate_items(passage: str, examples: list[dict]) -> str:
    msgs = build_prompt(passage, examples)
    resp = openai.chat.completions.create(
        model=CHAT_MODEL,
        temperature=TEMP,
        messages=msgs,
    )
    return resp.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â¸ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    passage = input("ğŸ” ìƒˆ ì§€ë¬¸ ì „ë¬¸ì„ ë¶™ì—¬ ë„£ìœ¼ì„¸ìš”:\n")

    # ì˜ˆì‹œ: ìƒìœ„ 50â€¯ê°œì—ì„œ MAX_EXAMPLESë§Œ ì‚¬ìš© (ê·¸ë£¹ í•„í„° ì œê±°)
    vec   = embed(passage)
    hits  = col.query(query_embeddings=[vec],
                      n_results=50,
                      include=["documents", "metadatas"])
    examples = [
        {"meta": m, "doc": d}
        for d, m in zip(hits["documents"][0], hits["metadatas"][0])
    ][:MAX_EXAMPLES]

    print(f"\nğŸ“‘ ì˜ˆì‹œ ì„¸íŠ¸ {len(examples)}ê°œ í™•ë³´ (MAX_EXAMPLES={MAX_EXAMPLES})")

    print("ğŸ› ï¸  LLMì— ìƒˆ ë¬¸ì œ ìƒì„± ìš”ì²­â€¦")
    output = generate_items(passage, examples)

    print("\nğŸ†• ìƒì„± ê²°ê³¼ â†“â†“â†“\n" + "="*60)
    print(output)
    print("="*60)

    ts  = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    Path(f"generated_items_{ts}.json").write_text(output, encoding="utf-8")