import os, glob, json, chromadb, tiktoken
import hashlib, re
import numpy as np
from scipy.spatial.distance import cosine
from kiwipiepy import Kiwi
import time
from sentence_transformers import SentenceTransformer

# â”€â”€ ê·¸ë£¹ í•´ì‹œ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def canonical_passage(item: dict) -> str:
    """passage ë˜ëŠ” context_boxë¥¼ ê³µë°± 1ì¹¸ìœ¼ë¡œ ì •ê·œí™”í•´ ë°˜í™˜"""
    txt = (item.get("passage") or item.get("context_box") or "")
    return " ".join(txt.split())

def passage_hash(item: dict, length: int = 12) -> str:
    """
    ë™ì¼ ì§€ë¬¸ì´ë©´ íŒŒì¼ëª…ì´ ë‹¬ë¼ë„ ë™ì¼ í•´ì‹œë¥¼ ì–»ë„ë¡ SHAâ€‘1 ê¸°ë°˜ ê·¸ë£¹í‚¤ ìƒì„±.
    ê¸°ë³¸ 12ì(48bit) â†’ ì¶©ëŒ í™•ë¥  1/2^48 â‰ˆ 1.4eâ€‘14
    """
    text = canonical_passage(item)
    return hashlib.sha1(text.encode("utf-8")).hexdigest()[:length]

# â”€â”€ ìœ ì‚¬ë„ ê³„ì‚°ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
kiwi = Kiwi()  # Pureâ€‘Python í˜•íƒœì†Œ ë¶„ì„ê¸° (Java ë¶ˆí•„ìš”)

def pos_set(text: str):
    "í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ í’ˆì‚¬ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•´ ì§‘í•©ìœ¼ë¡œ ë°˜í™˜"
    return {tok.tag for tok in kiwi.tokenize(text)}

def pos_jaccard(a_set: set, b_set: set) -> float:
    "í’ˆì‚¬ ì§‘í•© Jaccard ìœ ì‚¬ë„ (0~1)"
    if not a_set or not b_set:
        return 0.0
    return len(a_set & b_set) / len(a_set | b_set)


def cosine_sim(v1: list, v2: list) -> float:
    "ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (âˆ’1~1) â†’ 0~1 ì •ê·œí™”"
    return 1 - cosine(v1, v2)  # scipy ë°˜í™˜ì€ distance

# â”€â”€ ì½ê¸° ë‚œì´ë„ ì§€í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def readability_kor(text: str) -> float:
    """
    ê°„ì´ í•œêµ­ì–´ ì½ê¸° ë‚œì´ë„ ì ìˆ˜ (0=ì‰¬ì›€ â†’ 1=ì–´ë ¤ì›€).
    - í‰ê·  ë¬¸ì¥ ê¸¸ì´ì™€ í‰ê·  ì–´ì ˆ(í† í°) ê¸¸ì´ë¥¼ ê²°í•©.
    - í•„ìš”ì— ë”°ë¼ BLRDÂ·AI-KLE ë“± ì •ì‹ ì§€í‘œë¡œ êµì²´ ê°€ëŠ¥.
    """
    # ë¬¸ì¥ ë¶„ë¦¬
    sents = re.split(r"[\.\\?!\\n]", text.strip())
    sents = [s.strip() for s in sents if s.strip()]
    if not sents:
        return 0.0
    # í‰ê·  ë¬¸ì¥ ê¸¸ì´(ì–´ì ˆ ìˆ˜)
    sent_lens = [len(sent.split()) for sent in sents]
    avg_sent = sum(sent_lens) / len(sent_lens)
    # í‰ê·  ì–´ì ˆ ê¸¸ì´(ìŒì ˆ ìˆ˜)
    words = text.split()
    avg_word = sum(len(w) for w in words) / len(words) if words else 0
    # ê°€ì¤‘í•© (ì¡°ì • ê°€ëŠ¥)
    score = 0.6 * (avg_sent / 30) + 0.4 * (avg_word / 6)
    # 0~1 ë¡œ í´ë¦¬í•‘
    return round(min(max(score, 0), 1), 3)

# â”€â”€ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ max_tokens ë‹¨ìœ„ë¡œ ì²­í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import tiktoken
enc = tiktoken.get_encoding("cl100k_base")
MAX_TOK = 256  # embed ì²­í¬ ê¸¸ì´

def chunk_text(text: str, max_tokens: int = MAX_TOK):
    """
    ì£¼ì–´ì§„ ë¬¸ìì—´ì„ ìµœëŒ€ max_tokens í† í° ê¸¸ì´ë¡œ ë‚˜ëˆ  ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    ë¬¸ì¥ ê²½ê³„ë¥¼ ìš°ì„  ê³ ë ¤í•˜ë˜, ê¸¸ì´ ì´ˆê³¼ ì‹œ ê°•ì œë¡œ ìë¦„.
    """
    sents = re.split(r"(?<=[.!?\\n])", text)
    chunks, current = [], ""
    for s in sents:
        if not s.strip():
            continue
        if len(enc.encode(current + s)) <= max_tokens:
            current += s
        else:
            if current:
                chunks.append(current.strip())
            # ê¸¸ë©´ ë¬¸ì¥ ë‹¨ìœ„ ë¬´ì‹œí•˜ê³  hardâ€‘split
            while len(enc.encode(s)) > max_tokens:
                head_tokens = enc.encode(s)[:max_tokens]
                chunks.append(enc.decode(head_tokens))
                s = enc.decode(enc.encode(s)[max_tokens:])
            current = s
    if current:
        chunks.append(current.strip())
    return chunks or [text[:max_tokens]]

# â”€â”€ â¶ ê²½ë¡œ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SRC_DIR = "/Users/stillclie_mac/Documents/ug/snoriginal/db"
DB_PATH = "./sn_csat_2.db"         # Chroma í¼ì‹œìŠ¤í„´ìŠ¤ ë””ë ‰í„°ë¦¬(í´ë”ëª…)
COL_NAME = "sn_csat_openai"        # ê¸°ì¡´ ì»¬ë ‰ì…˜ëª… ìœ ì§€ (ë³€ê²½ ì›í•˜ë©´ ì´ ê°’ë§Œ ìˆ˜ì •)

# â”€â”€ â· ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ë¡œì»¬ SentenceTransformer) â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ (í™˜ê²½ë³€ìˆ˜ EMBED_MODEL ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
EMBED_MODEL = os.environ.get("EMBED_MODEL", "nlpai-lab/KURE-v1")
print(f"ğŸ”§  Using embedding model: {EMBED_MODEL}")

# SentenceTransformer ë¡œì»¬ ëª¨ë¸ ë¡œë“œ
# normalize_embeddings=True ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì½”ì‚¬ì¸/ìœ í´ë¦¬ë“œ ì¼ê´€ì„± í™•ë³´
# Force CPU to avoid Apple MPS scratchâ€‘pad OOM
_model = SentenceTransformer(EMBED_MODEL, device="cpu")
# Limit sequence length so attention buffer stays small
try:
    _model.max_seq_length = 256
except AttributeError:
    pass

# â”€â”€ â¸ Chroma ì»¬ë ‰ì…˜ ì˜¤í”ˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client = chromadb.PersistentClient(path=DB_PATH)
col = client.get_or_create_collection(
    COL_NAME, metadata={"hnsw:space": "cosine"}
)

# â”€â”€ ìœ í‹¸: ì§€ë¬¸+ë¬¸í•­+ì„ íƒì§€ í•©ì¹˜ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def merge_text(item: dict) -> str:
    """
    ì§€ë¬¸ + ë¬¸ì œ + ì„ íƒì§€ë¥¼ í•œ ë¬¸ìì—´ë¡œ
    None ì´ë‚˜ ëˆ„ë½ í•„ë“œëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•´ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•œë‹¤.
    """
    passage = item.get("passage") or item.get("context_box") or ""
    question = item.get("question") or ""
    choices = " ".join(opt.get("text", "") for opt in item.get("options", []))
    return f"{passage}\n{question}\n{choices}"

# â”€â”€ â¹ ëª¨ë“  JSON íŒŒì¼ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
files = sorted(glob.glob(os.path.join(SRC_DIR, "*.json")))
print(f"ğŸ”  Found {len(files)} JSON files.")

pos_sets = []  # passageë³„ í’ˆì‚¬ ì§‘í•© ë³´ê´€
ids, docs, metas = [], [], []
for path in files:
    with open(path, encoding="utf-8") as f:
        item = json.load(f)

    # ì§ˆë¬¸(question)ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    if not item.get("question"):
        print(f"âš ï¸  Skip {path} (missing question)")
        continue

    ids.append(item["id"])  # ê³ ìœ  IDëŠ” ê¸°ì¡´ JSONì˜ id ì‚¬ìš©
    docs.append(merge_text(item))

    # í’ˆì‚¬ ì§‘í•© ì €ì¥ (ì§€ë¬¸/contextë§Œ ì‚¬ìš©)
    passage_text = item.get("passage") or item.get("context_box") or ""
    pos_sets.append(pos_set(passage_text))

    # ë©”íƒ€ë°ì´í„°ì—ì„œ None, dict, list íƒ€ì… ê°’ì„ ì œê±°(ChromaëŠ” dict/list í—ˆìš©í•˜ì§€ ì•ŠìŒ)
    clean_meta = {}
    for k, v in item.items():
        if k in ("passage", "question", "options"):
            continue
        if v is None:
            continue
        # primitive íƒ€ì…ë§Œ í—ˆìš©
        if isinstance(v, (str, int, float, bool)):
            clean_meta[k] = v
    # ì½ê¸° ë‚œì´ë„
    clean_meta["reading_level"] = readability_kor(passage_text)
    # â€• ê·¸ë£¹ í•´ì‹œ ì¶”ê°€ â€•
    clean_meta["group"] = passage_hash(item)
    # ì›ë³¸ JSON íŒŒì¼ ê²½ë¡œ ì €ì¥
    clean_meta["file_path"] = path
    metas.append(clean_meta)

# â”€â”€ âº ì„ë² ë”© í•¨ìˆ˜ (ë¡œì»¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def embed(batch, batch_size: int = 64):
    """SentenceTransformer(â‰¤256 tokens) CPU ì„ë² ë”© â†’ list[list[float]] ë°˜í™˜"""
    vecs = _model.encode(
        batch,
        batch_size=batch_size,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    return vecs.tolist()

# â”€â”€ â» ì„ë² ë”© (ì²­í¬â€‘í‰ê· ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embs = []
for idx, doc in enumerate(docs, 1):
    chunks = chunk_text(doc, MAX_TOK)
    chunk_vecs = embed(chunks, batch_size=4)  # ì†Œì²­í¬ ë°°ì¹˜
    # í‰ê·  í’€ë§
    avg_vec = np.mean(chunk_vecs, axis=0).tolist()
    embs.append(avg_vec)
    if idx % 20 == 0 or idx == len(docs):
        print(f"  â†’ Embedded {idx}/{len(docs)} docs ({len(chunks)} chunks last)")

# â”€â”€ â¼ ì˜ë¯¸Â·í˜•ì‹ ìµœëŒ€ ìœ ì‚¬ë„ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
max_sem_sims = []
max_struct_sims = []

for i, emb_i in enumerate(embs):
    if i == 0:  # ì²« í•­ëª©ì€ ìê¸° ìì‹ ë¿
        max_sem_sims.append(1.0)
        max_struct_sims.append(1.0)
        continue

    sem_scores = [cosine_sim(emb_i, embs[j]) for j in range(i)]
    struct_scores = [pos_jaccard(pos_sets[i], pos_sets[j]) for j in range(i)]

    max_sem_sims.append(max(sem_scores))
    max_struct_sims.append(max(struct_scores))

# ë©”íƒ€ë°ì´í„°ì— ìœ ì‚¬ë„ ê¸°ë¡
for i, meta in enumerate(metas):
    meta["max_sem_sim"] = round(max_sem_sims[i], 4)
    meta["max_struct_sim"] = round(max_struct_sims[i], 4)

# â”€â”€ â½ Chroma ì»¬ë ‰ì…˜ì— ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
col.add(ids=ids, documents=docs, embeddings=embs, metadatas=metas)
print(f"âœ…  {len(ids)} items stored in {DB_PATH}:{COL_NAME}")